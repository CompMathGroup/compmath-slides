\documentclass[professionalfonts,unicode]{beamer}

\usepackage{amsmath,amssymb}
\usepackage[utf8]{inputenc}

\usepackage[russian]{babel}

\usepackage{ifthen}

\usetheme{Warsaw}
\usecolortheme{uranix}

\setbeamertemplate{headline}
{%
  \begin{beamercolorbox}[sep=0.3cm,wd=\paperwidth]{section in head/foot}%
    \usebeamerfont{frametitle}%
    \vbox{}\vskip-1ex%
    \strut\insertsectionhead\strut\par%
    \vskip-1ex%
  \end{beamercolorbox}%
}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{}
\setbeamertemplate{caption}[numbered]

\graphicspath{{images//}}

\title[СЛАУ]{Подготовка к контрольной. Системы линейных алгебраичеких уравнений}
\author[Цыбулин И.В.]{Скалько Юрий Иванович\\
\textbf{Цыбулин Иван}
\\Шевченко Александр}
\date{}
%\vspace{0.3cm}

\newcommand\mypar{\medskip\newline}

\newcommand\myframe[3][dup]{
\ifthenelse{\equal{#1}{}}{}{\ifthenelse{\equal{#1}{dup}}{\subsection{#2}}{\subsection{#1}}}
\frame{\frametitle{#2}{#3}}%
}

\newcommand{\sign}{\operatorname{sgn}}

\begin{document}

{
\setbeamertemplate{headline}[default]
\frame{\titlepage}
}

\section{LU-разложение}
\myframe{LU-подобное разложение}
{
	Представить матрицу
	$$
	A = \begin{pmatrix}
		11&-3&6\\
		-3&7&0\\
		6&0&7
	\end{pmatrix}
	$$
	в виде произведения нижней треугольной матрицы $L$ и верхней треугольной матрицы с единичной матрицей $U$.
	\mypar
	От ``настоящего'' $LU$-разложения данное разложение отличается требованием единичной диагонали у матрицы $U$,
	когда для традиционного единичную диагональ имеет матрица $L$.
}

\myframe[]{LU-подобное разложение}
{
	Представить матрицу
	$$
	A = \begin{pmatrix}
		11&-3&6\\
		-3&7&0\\
		6&0&7
	\end{pmatrix}
	$$
	\mypar
	Будем искать $L$ и $U$ в виде
	$$
	L = \begin{pmatrix}
		l_{11}&0&0\\
		l_{21}&l_{22}&0\\
		l_{31}&l_{32}&l_{33}
	\end{pmatrix}
	\qquad
	U = \begin{pmatrix}
		1&u_{12}&u_{13}\\
		0&1&u_{23}\\
		0&0&1
	\end{pmatrix}	
	$$
	$$
	\begin{pmatrix}
		l_{11}&0&0\\
		l_{21}&l_{22}&0\\
		l_{31}&l_{32}&l_{33}
	\end{pmatrix}
	\begin{pmatrix}
		1&u_{12}&u_{13}\\
		0&1&u_{23}\\
		0&0&1	
	\end{pmatrix}	=
	\begin{pmatrix}
		11&-3&6\\
		-3&7&0\\
		6&0&7
	\end{pmatrix}
	$$
}

\myframe[]{LU-подобное разложение}
{
	\parbox[c][0.3\textheight]{\textwidth}{
	$$
	\begin{pmatrix}
		l_{11}&0&0\\
		l_{21}&l_{22}&0\\
		l_{31}&l_{32}&l_{33}
	\end{pmatrix}
	\begin{pmatrix}
		1&u_{12}&u_{13}\\
		0&1&u_{23}\\
		0&0&1	
	\end{pmatrix}	=
	\begin{pmatrix}
		11&-3&6\\
		-3&7&0\\
		6&0&7
	\end{pmatrix}
	$$
	}
	\parbox[c][0.3\textheight]{\textwidth}{
	Для начала найдем первый столбец матрицы $L$ и первую стоку матрицы $U$
	$$
	l_{i1} = a_{i1}, \quad l_{11} u_{1j} = a_{1j}
	$$
	}
	\parbox[c][0.3\textheight]{\textwidth}{
	$$
	\begin{pmatrix}
		11&0&0\\
		-3&l_{22}&0\\
		6&l_{32}&l_{33}
	\end{pmatrix}
	\begin{pmatrix}
		1&-3/11&6/11\\
		0&1&u_{23}\\
		0&0&1	
	\end{pmatrix}	=
	\begin{pmatrix}
		11&-3&6\\
		-3&7&0\\
		6&0&7
	\end{pmatrix}
	$$
	}
}

\myframe[]{LU-подобное разложение}
{
	\parbox[c][0.3\textheight]{\textwidth}{
	$$
	\begin{pmatrix}
		11&0&0\\
		-3&l_{22}&0\\
		6&l_{32}&l_{33}
	\end{pmatrix}
	\begin{pmatrix}
		1&-3/11&6/11\\
		0&1&u_{23}\\
		0&0&1	
	\end{pmatrix}	=
	\begin{pmatrix}
		11&-3&6\\
		-3&7&0\\
		6&0&7
	\end{pmatrix}
	$$
	}
	\parbox[c][0.3\textheight]{\textwidth}{
	Для второй строки $U$ и второго столбца $L$ имеются уравнения
	\begin{align*}
	9/11&+l_{22} = a_{22}\\
	-18/11&+l_{32} = a_{32}\\
	-18/11&+l_{22}u_{23} = a_{23}
	\end{align*}
	}
	\parbox[c][0.3\textheight]{\textwidth}{
	$$
	\begin{pmatrix}
		11&0&0\\
		-3&68/11&0\\
		6&18/11&l_{33}
	\end{pmatrix}
	\begin{pmatrix}
		1&-3/11&6/11\\
		0&1&9/34\\
		0&0&1	
	\end{pmatrix}	=
	\begin{pmatrix}
		11&-3&6\\
		-3&7&0\\
		6&0&7
	\end{pmatrix}
	$$
	}
}

\myframe[]{LU-подобное разложение}
{
	\parbox[c][0.3\textheight]{\textwidth}{
	$$
	\begin{pmatrix}
		11&0&0\\
		-3&68/11&0\\
		6&18/11&l_{33}
	\end{pmatrix}
	\begin{pmatrix}
		1&-3/11&6/11\\
		0&1&9/34\\
		0&0&1	
	\end{pmatrix}	=
	\begin{pmatrix}
		11&-3&6\\
		-3&7&0\\
		6&0&7
	\end{pmatrix}
	$$
	}
	\parbox[c][0.3\textheight]{\textwidth}{
	Последнее уравнение на $l_{33}$
	$$
	36/11+ 81/187 + l_{33} = a_{33} = 7
	$$
	}
	\parbox[c][0.3\textheight]{\textwidth}{
	$$
	\begin{pmatrix}
		11&0&0\\
		-3&68/11&0\\
		6&18/11&56/17
	\end{pmatrix}
	\begin{pmatrix}
		1&-3/11&6/11\\
		0&1&9/34\\
		0&0&1	
	\end{pmatrix}	=
	\begin{pmatrix}
		11&-3&6\\
		-3&7&0\\
		6&0&7
	\end{pmatrix}
	$$
	}	
}

\section{Матричные нормы}
\myframe{Норма матрицы}
{
	Норма матрицы определяется  через некоторую векторную норму $\|\cdot\|$ следующим образом:
	$$
	\|A\| = \sup_{\|x\| \neq 0} \|Ax\|
	$$
	Свойства нормы матрицы
	\begin{align*}
	\|Ax\| &\leq \|A\|\|x\|\\
	\exists x_0 \neq 0: \|Ax_0\| &= \|A\|\|x_0\|
	\end{align*}
	
	Рассматриваемые векторные нормы
	\begin{itemize}
		\item $\|x\|_\infty = \max |x_i|$ --- ``Первая'' или $\ell_\infty$ норма
		\item $\|x\|_1 = \sum |x_i|$ --- ``Вторая'' или $\ell_1$ норма
		\item $\|x\|_E = \sqrt{\sum x_i^2}$ --- ``Третья'' или евклидова норма
	\end{itemize}
}

\myframe{Первая норма}
{
	$$
	\|x\|_\infty = \max |x_i|
	$$
	Соответствующая матричная норма
	$$
	\|A\|_\infty = \max_i \sum_j |a_{ij}|
	$$
	Пусть максимум достигается для $i_0$-й строки. Тогда на векторе
	$$
	x_0 = \alpha\begin{pmatrix}
		\sign a_{i_01}\\
		\sign a_{i_02}\\
		\vdots\\
		\sign a_{i_0n}
	\end{pmatrix}, \quad \|x_0\|_\infty = |\alpha|
	$$
	достигается норма матрицы, т.е.
	$$
	\|Ax_0\|_\infty = \|A\|_\infty\|x_0\|_\infty
	$$
}

\myframe{Вторая норма}
{
	$$
	\|x\|_1 = \sum |x_i|
	$$
	Соответствующая матричная норма
	$$
	\|A\|_1 = \max_j \sum_i |a_{ij}|
	$$
	Пусть максимум достигается для $j_0$-го столбца. Тогда на векторе
	$$
	x_0 = \left\{
	\begin{array}{l@{}cl}
		0&,& i \neq j_0\\
		\alpha&,& i = j_0\\
	\end{array}
	\right., \quad \|x_0\|_1 = |\alpha|
	$$
	достигается норма матрицы, т.е.
	$$
	\|Ax_0\|_1 = \|A\|_1\|x_0\|_1
	$$
}

\myframe{Евклидова норма. Несимметричный случай}
{
	$$
	\|x\|_E = \sqrt{\sum x_i^2}
	$$
	Соответствующая матричная норма
	$$
	\|A\|_E = \sqrt{\max \lambda(A^TA)}
	$$
	Пусть $x_0$ --- собственный вектор матрицы $A^TA$, соответствующий максимальному собственному значению
	$$
	A^TAx_0 = \lambda_{\operatorname{max}} x_0
	$$
	Тогда на $x_0$ достигается норма матрицы
	$$
	\|Ax_0\|_E = \|A\|_E\|x_0\|_E
	$$
}

\myframe{Евклидова норма. Симметричный случай}
{
	$$
	\|x\|_E = \sqrt{\sum x_i^2}
	$$
	В случае $A = A^T$, матричная норма
	$$
	\|A\|_E = \max |\lambda(A)|
	$$
	Пусть $x_0$ --- собственный вектор матрицы $A$, соответствующий максимальному по модулю собственному значению
	$$
	Ax_0 = \lambda_{\operatorname{absmax}} x_0
	$$
	Тогда на $x_0$ достигается норма матрицы
	$$
	\|Ax_0\|_E = \|A\|_E\|x_0\|_E
	$$
}

\section{Линейные системы}
\myframe{Обусловленность}
{
	Для системы линейных уравнений
	\begin{align*}
	Ax &= b\\
	A(x+\Delta x) &= b + \Delta b
	\end{align*}
	справедливо следующее соотношение между относительной погрешностью правой части и относительной погрешностью решения
	$$
	\frac{\|\Delta x\|}{\|x\|} \leq \nu(A,b) \frac{\|\Delta b\|}{\|b\|} \leq \mu(A) \frac{\|\Delta b\|}{\|b\|}
	$$
	Здесь
	$$
	\nu(A,b) = \frac{\|A^{-1}\|\|b\|}{\|A^{-1}b\|}, \qquad \mu(A) = \|A\|\|A^{-1}\|
	$$
}

\myframe{Точные оценки сверху}
{
	При условии, что правая часть $b$ задана, оценка
	$$
	\frac{\|\Delta x\|}{\|x\|} \leq \nu(A,b) \frac{\|\Delta b\|}{\|b\|}
	$$
	является точной, поскольку при некотором $\Delta b$ она обращается в равенство, то есть погрешность достигается.
	Это происходит при 
	$$
	\|A^{-1} \Delta b\| = \|A^{-1}\|\|\Delta b\|
	$$
	то есть на векторе $\Delta b$  достигается норма матрицы $A^{-1}$
}

\myframe[]{Точные оценки сверху}
{
	При условии, что правая часть $b$ произвольна, оценка
	$$
	\frac{\|\Delta x\|}{\|x\|} \leq \mu(A) \frac{\|\Delta b\|}{\|b\|}
	$$
	является точной, поскольку при некоторых $\Delta b$ и $b$ она обращается в равенство, то есть погрешность достигается.
	Это происходит при 
	\begin{align*}
	\|A^{-1} \Delta b\| &= \|A^{-1}\|\|\Delta b\|\\
	\|Ax\| = \|b\|  &= \|A\| \|A^{-1}b\| = \|A\|\|x\|
	\end{align*}	
	то есть на векторе $\Delta b$  достигается норма матрицы $A^{-1}$, на векторе $x$ достигается норма $A$. Соответствующий вектор $b = Ax$
}

\myframe{Оценки снизу}
{
	В случае когда интересует оценка снизу для минимально возможной погрешности, можно поступить следующим образом.
	Рассмотрим систему, эквивалентную исходной
	$$
	A^{-1} b = x
	$$
	Для нее справедливо
	$$
	\frac{\|\Delta b\|}{\|b\|} \leq \nu(A^{-1},x) \frac{\|\Delta x\|}{\|x\|} \leq \mu(A^{-1}) \frac{\|\Delta x\|}{\|x\|}
	$$
	Учитывая $\mu(A) = \mu(A^{-1})$
	$$
	\frac{\|\Delta x\|}{\|x\|} \geq \frac{1}{\nu(A^{-1},x)}\frac{\|\Delta b\|}{\|b\|} \geq \frac{1}{\mu(A)}\frac{\|\Delta b\|}{\|b\|}
	$$	
}

\myframe{Точные оценки снизу}
{
	По аналогии со случаем верхних оценок, при некотором $\Delta b$ оценка 
	$$
	\frac{\|\Delta x\|}{\|x\|} \geq \frac{1}{\nu(A^{-1},x)}\frac{\|\Delta b\|}{\|b\|}
	$$	
	достигается. Это следует из того, что оценка
	$$
	\frac{\|\Delta b\|}{\|b\|} \leq \nu(A^{-1},x) \frac{\|\Delta x\|}{\|x\|}
	$$
	для системы $A^{-1}b = x$ превращается в равенство при
	$$
	\|A \Delta x\| = \|\left(A^{-1}\right)^{-1} \Delta x\| = \|\left(A^{-1}\right)^{-1}\|\|\Delta x\| = \|A\|\|\Delta x\|
	$$
	Эта оценка становится точной когда на $\Delta x$ достигается норма $A$. Соответствующее $\Delta b = A \Delta x$
}

\myframe[]{Точные оценки снизу}
{
	По аналогии со случаем верхних оценок, при некоторых $\Delta b$,$b$ оценка 
	$$
	\frac{\|\Delta x\|}{\|x\|} \geq \frac{1}{\mu(A)}\frac{\|\Delta b\|}{\|b\|}
	$$	
	достигается. Это следует из того, что оценка
	$$
	\frac{\|\Delta b\|}{\|b\|} \leq \mu(A^{-1}) \frac{\|\Delta x\|}{\|x\|}
	$$
	для системы $A^{-1}b = x$ превращается в равенство при
	\begin{align*}
	\|A \Delta x\| = \|\left(A^{-1}\right)^{-1} \Delta x\| &= \|\left(A^{-1}\right)^{-1}\|\|\Delta x\| = \|A\|\|\Delta x\|\\
	\|A^{-1}b\| &= \|A^{-1}\| \|b\|
	\end{align*}
	
	Эта оценка становится точной когда на $\Delta x$ достигается норма $A$, а на векторе $b$ достигается норма $A^{-1}$. 
	Соответствующее $\Delta b = A \Delta x$
}

\section{Итерационные методы}
\myframe{Итерационный процесс}
{
	Итерационный процесс
	$$
	x_{n+1} = B x_n + f
	$$
	сходится если
	\begin{itemize}
		\item Некоторая норма $B$ меньше единицы (достаточное условие)
		\item Все собственные числа $\lambda(B)$ лежат в единичном круге (необходимое и достаточное условие)
	\end{itemize}
	
	Если все собственные значения $B$ различны, то скорость сходимости 
	\begin{align*}
	q = \max |\lambda(B)|
	\end{align*}
	
	Матрица $B$ часто бывает несимметричной, поэтому собственные числа могут быть комплексными.
}

\myframe{Монотонная и немонотонная сходимость}
{
	Пусть итерационный процесс
	$$
	x_{n+1} = B x_n + f
	$$
	сходится к $x^*$. Рассмотрим ошибку $\varepsilon_n = x_n - x^*$. Для ошибки справедливо
	$$
	\varepsilon_{n+1} = B \varepsilon_n
	$$
	Возможны 2 случая:
	\begin{itemize}
		\item $\|B\| < 1$. Сходимость имеет монотонный характер в силу
		$$
		\|\varepsilon_{n+1}\| \leq \|B\| \|\varepsilon_n\| < \|\varepsilon_n\|
		$$
		\item $\|B\| \geq 1$. Существует начальное приближения, для которого сходимость имеет немонотонный характер.
		Возьмем $x_0 = x^* + \varepsilon_0$, причем на $\varepsilon_0$ достигается норма $B$
		$$
		\| \varepsilon_1 \| = \|B \varepsilon_0 \| = \|B\| \|\varepsilon_0\| > \|\varepsilon_0\|
		$$
		После первой итерации норма ошибки решения возрастает, сходимость немонотонная
	\end{itemize}
}

\myframe{Метод Якоби}
{
	Итерационный метод Якоби для системы
	$$
	Ax = b
	$$
	выглядит как
	$$
	x_{n+1} = x_n - D^{-1}(Ax_n-b) = (E - D^{-1}A) x_n + D^{-1}b
	$$
	Матрица $D$ состоит из диагональных элементов $A$
	$$
	D = \operatorname{diag}(a_{ii})
	$$
	
	Метод Якоби сходятся для матриц с диагональным преобладанием. При отсутствии преобладания необходимо проверять
	собственные значения $B = E - D^{-1}A$ или искать норму в которой $\|B\| < 1$
}

\myframe{Метод Зейделя}
{
	Итерационный метод Зейделя для системы
	$$
	Ax = b
	$$
	выглядит как
	$$
	x_{n+1} = L^{-1}(-Ux_n + b) = -L^{-1}U x_n + L^{-1}b
	$$
	Матрица $U$ состоит из элементов выше главной диагонали, $L$ --- из остальных
	$$
	U = \left\{
	\begin{array}{l@{}cl}
		a_{ij}&,&j>i\\
		0&,&j\leq i
	\end{array}
	\right.
	\qquad
	L = \left\{
	\begin{array}{l@{}cl}
		0&,&j> i\\
		a_{ij}&,&j\leq i
	\end{array}
	\right.
	$$
	
	Метод Зейделя сходится при условии $A = A^T > 0$, иначе необходимо проверять
	собственные значения $B = -L^{-1}U$ или искать норму в которой $\|B\| < 1$
}

\myframe{Метод простой итерации с параметром}
{
	Для уравнения $Ax = b$ метод простой итерации записывается как
	$$
	x_{n+1} = x_n - \tau(Ax_n - b) = (E - \tau A) x_n + \tau b
	$$
	В случае $A = A^T > 0$ все собственные числа $\lambda(A) > 0$.
	Собственные значения матрицы $B$
	$$
	\lambda(B) = \lambda(E - \tau A) = 1 - \tau \lambda(A)
	$$
	
	Итерационный процесс сходится, если все $\lambda(B)$ по модулю меньше $1$
	$$
	|1 - \lambda(A)| < 1 \Leftrightarrow \tau < \frac{2}{\lambda_{\operatorname{max}}(A)}
	$$
}

\myframe[]{Метод простой итерации с параметром}
{
	Скорость сходимости $q$ метода простой итерации определяется
	минимальным и максимальным собственными числами $A$.
	$$
	q = \max |\lambda(B)| = \max(|1-\tau \lambda_{\operatorname{min}}|,|1-\tau \lambda_{\operatorname{max}}|)
	$$
	
	При $\tau = \frac{2}{\lambda_{\operatorname{min}}+\lambda_{\operatorname{max}}}$ $q$ принимает минимальное значение
	$$
	q_{\operatorname{opt}} = 
	\frac
	{\lambda_{\operatorname{max}}-\lambda_{\operatorname{min}}}
	{\lambda_{\operatorname{max}}+\lambda_{\operatorname{min}}}
	$$
}

\myframe{Специальные начальные условия}
{
	В случае, когда начальная ошибка $\varepsilon_0 = x_0 - x^*$ раскладывается по части собственных векторов матрицы $A$, 
	собственные значения, соответствующие остальным собственным векторам можно исключить при нахождении скорости сходимости.
	\mypar
	Например, если в разложении $\varepsilon_0$ по собственным векторам нет слагаемого соответствующего собственному вектору
	с максимальным собственным значением, то в качестве $\lambda_{\operatorname{max}}$ берется максимальное из оставшихся значение
}

\myframe[]{Специальные начальные условия}
{
	Пусть у матрицы $A$ собственные значения $\lambda_1, \lambda_2$ и $\lambda_3$
	$$
	A h_i = \lambda_i h_i, \quad, \lambda_1 < \lambda_2 < \lambda_3
	$$
	а начальная ошибка $x_0 = x^*$ раскладывается только по $h_1$ и $h_2$. Скорость сходимости при данном $\tau$
	теперь определяется только значениями $\lambda_1$ и $\lambda_2$
	$$
	q = \max(|1 - \tau \lambda_1|, |1 - \tau \lambda_2|)
	$$
	Условие $q < q_{\operatorname{opt}}$ задает границы параметра $\tau$ при которых скорость сходимости для данного приближения 
	выше чем при $\tau_{\operatorname{opt}}$
	$$
	\frac{1-q_{\operatorname{opt}}}{\lambda_1} < \tau < \frac{1+q_{\operatorname{opt}}}{\lambda_2}
	$$
}

\myframe[]{Специальные начальные условия}
{
	Хотя при вычислении скорости сходимости некоторые собственные значения могут отбрасываться, условие
	$$
	\tau < \frac{2}{\lambda_{\operatorname{max}}}
	$$
	должно быть выполнено всегда, даже если в исходной ошибке не было соответствующей компоненты в разложении
	по собственным векторам.
	\mypar
	В случае нарушения этого условия процесс становится вычислительно неустойчивым. Условие
	$$
	\frac{1-q_{\operatorname{opt}}}{\lambda_1} < \tau < \frac{1+q_{\operatorname{opt}}}{\lambda_2}
	$$
	следует исправить на
	$$
	\frac{1-q_{\operatorname{opt}}}{\lambda_1} < \tau < \min\left(\frac{1+q_{\operatorname{opt}}}{\lambda_2}, \frac{2}{\lambda_3}\right)
	$$
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%                            %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{
\setbeamertemplate{headline}[default] 
\frame{
	\begin{center}
	{\Huge Спасибо за внимание!}
	\end{center}
	\bigskip
	\begin{center}
	{\color{blue}{tsybulinhome@gmail.com}}
	\end{center}
	}
}

\end{document}
