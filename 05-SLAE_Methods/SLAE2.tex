\documentclass[apectratio=43,unicode]{beamer}
\usetheme{Moscow}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[main=russian,english]{babel}

\usepackage{amsmath,amssymb}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\hypersetup{
	pdfauthor={Ivan Tsybulin}
}

\usepackage{euler}
\usepackage{mathtools}

\graphicspath{{images//}}

\title[СЛАУ]{Системы линейных алгебраических уравнений\\Часть 2. Прямые и итерационные методы решения}
\author[Цыбулин И.В.]{Скалько Юрий Иванович\\
\textbf{Цыбулин Иван}}
\date{}

\newcommand{\colorhref}[2]{\href{#1}{\textcolor{miptbase!30!black}{#2}}}

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}

\section{ }

\def\A{\mathbf{A}}
\def\B{\mathbf{B}}
\def\C{\mathbf{C}}
\def\D{\mathbf{D}}
\def\E{\mathbf{E}}
\def\L{\mathbf{L}}
\def\U{\mathbf{U}}
\def\S{\mathbf{S}}
\def\J{\mathbf{J}}
\def\x{\mathbf{x}}
\def\y{\mathbf{y}}
\def\v{\mathbf{v}}
\def\w{\mathbf{w}}
\def\f{\mathbf{f}}
\def\b{\mathbf{b}}
\def\r{\mathbf{r}}
\def\p{\mathbf{p}}
\def\z{\mathbf{z}}
\newcommand{\tr}{\mathsf{T}}

\section{Методы решения СЛАУ}
\begin{frame}\frametitle{Прямые и итерационные методы}
	Численные методы решения СЛАУ делятся на 2 типа:
	\begin{itemize}
	\item{Прямые} --- решение системы получается за конечное число действий
	\item{Итерационные} --- решение строится в виде последовательности приближений к решению.
	При достижении заданной точности итерации прекращают
	\end{itemize}
\end{frame}

\section{Прямые методы}
\subsection{Метод Гаусса}
\begin{frame}\frametitle{Метод Гаусса}
	В основе метода лежит последовательное исключение неизвестных. Метод Гаусса состоит из
	\emph{прямого хода}, на котором матрица системы элементарными преобразованиями строк преобразуется
	к верхнетреугольному виду, и \emph{обратного хода}, когда полученная треугольная система решается
	подстановкой.

	Метод Гаусса \emph{корректен} (в ходе решения не придется делить на ноль),
	если все главные миноры матрицы отличны от нуля.

	\pause
	На практике, иногда требуется переставлять строки или столбцы матрицы, чтобы избежать деления на ноль.
	Также, переставляя стоки и столбцы можно улучшить устойчивость метода, то есть чувствительность к ошибкам округления
\end{frame}

\begin{frame}\frametitle{Метод Гаусса}
	Рассмотрим систему
	\[
	\begin{pmatrix}
		10^{-3}&1\\
		1&-1
	\end{pmatrix}
	\begin{pmatrix}
		x_1\\x_2
	\end{pmatrix} =
	\begin{pmatrix}
		2\\1
	\end{pmatrix}
	\qquad
	\begin{pmatrix}
		x_1\\x_2
	\end{pmatrix} \approx
	\begin{pmatrix}
		2.997003\\
		1.997003\\
	\end{pmatrix}
	\]

	Будем решать ее методом Гаусса, используя в вычислениях только три значащие цифры
	\[
	\left(
	\begin{array}{@{}cc|c@{}}
		10^{-3}&1&2\\
		1&-1&1
	\end{array}
	\right)
	\sim
	\left(
	\begin{array}{@{}cc|c@{}}
		1&10^3&2\cdot10^3\\
		1&-1&1
	\end{array}
	\right)
	\stackrel{!}{\sim}
	\left(
	\begin{array}{@{}cc|c@{}}
		1&10^3&2\cdot10^3\\
		0&-10^3&-2\cdot10^3\\
	\end{array}
	\right)
	\sim
	\]
	\[
	\left(
	\begin{array}{@{}cc|c@{}}
		1&10^3&2\cdot10^3\\
		0&-10^3&-2\cdot10^3\\
	\end{array}
	\right)
	\sim
	\left(
	\begin{array}{@{}cc|c@{}}
		1&10^3&2\cdot10^3\\
		0&1&2\\
	\end{array}
	\right)
	\sim
	\left(
	\begin{array}{@{}cc|c@{}}
		1&0&0\\
		0&1&2\\
	\end{array}
	\right)
	\]
	Полученное решение значительно отличается от точного.
\end{frame}

\begin{frame}\frametitle{Метод Гаусса с выбором главного элемента}
	Данную проблему можно устранить, если в качестве ведущего элемента (того на который делится очередная строка)
	выбирать наибольший по модулю элемент в оставшейся подматрице. Такой метод
	называется методом Гаусса с выбором главного элемента.

	Применим его к той же системе
	\[
	\left(
	\begin{array}{@{}cc|c@{}}
		10^{-3}&1&2\\
		\fbox{1}&-1&1
	\end{array}
	\right)
	\stackrel{!}{\sim}
	\left(
	\begin{array}{@{}cc|c@{}}
		0&\fbox{1}&2\\
		1&-1&1\\
	\end{array}
	\right)
	\sim
	\left(
	\begin{array}{@{}cc|c@{}}
		0&1&2\\
		1&0&3\\
	\end{array}
	\right)
	\sim
	\left(
	\begin{array}{@{}cc|c@{}}
		1&0&3\\
		0&1&2\\
	\end{array}
	\right)
	\]

	Теперь отличие от точного решения не превосходит $3 \cdot 10^{-3}$, что
	оказалось даже меньше погрешности вычислений.
\end{frame}

\begin{frame}\frametitle{Обусловленность}
	Исследуем, насколько плохо обусловлена матрица этой системы
	\[
	\A=\begin{pmatrix}
		10^{-3}&1\\
		1&-1
	\end{pmatrix}
	\qquad
	\A^{-1}=\frac{1}{1001}\begin{pmatrix}
		1000&1000\\
		1000&-1
	\end{pmatrix}
	\]
	\pause
	\[
	\mu_\infty(\A) = \|\A\|_\infty\|\A^{-1}\|_\infty = 2 \cdot \frac{2000}{1001} \approx 4
	\]

	Число обусловленности матрицы невелико, и хорошим численным методом можно решить данную систему с относительной ошибкой,
	не превосходящей $4 \frac{\delta \b}{\b}$.

	Следовательно, потеря точности при решении методом Гаусса без выбора главного элемента связана
	с самим методом, а не с плохой постановкой задачи. К тому же, метод Гаусса с выбором главного
	элемента нашел решение, погрешность которого вполне соответствует неустранимой ошибке
\end{frame}

\begin{frame}\frametitle{Вычислительная устойчивость}

	Проблема метода Гаусса заключается в накоплении ошибок округления. Эта проблема возникает всегда при
	операциях с числами разных знаков или разных порядков. Метод с выбором главного элемента позволяет
	уменьшить ошибку, но не всегда этого оказывается достаточно.

	\begin{block}{Теорема об устойчивости метода Гаусса}
		Если матрица $\A$ имеет диагональное преобладание
		\[
			|a_{ii}| \geq \sum_{j \neq i} |a_{ij}|,\quad i = \overline{1,n},
		\]
		то при решении методом Гаусса погрешность не превосходит неустранимой погрешности, то есть
		метод Гаусса \emph{вычислительно устойчив}
	\end{block}
\end{frame}

\subsection{Метод прогонки}
\begin{frame}\frametitle{Метод прогонки}
	Существует вариант метода Гаусса для важных в приложениях \emph{трехдиагональных} систем линейных уравнений
	\[
	\left\{
	\begin{array}{@{}c@{}c@{}c@{}c@{}c@{}c@{}ccc}
	b_1 x_1 &+& c_1 x_2 &&   && &=& f_1\\
	a_2 x_1 &+& b_2 x_2 &+& c_2 x_3 && &=& f_2\\
	&&&\ddots\\
	&& a_{n-1} x_{n-2} &+& b_{n-1} x_{n-1} &+& c_{n-1} x_n &=& f_{n-1}\\
	&&&& a_{n} x_{n-1} &+& b_{n} x_{n} &=& f_{n}\\
	\end{array}
	\right.
	\]
\end{frame}

\begin{frame}\frametitle{Вычисление прогоночных коэффициентов}
	Решение ищется в виде прогоночного соотношения
	\[
	x_{i} = P_{i} x_{i+1} + Q_{i}
	\]
	При подстановке его в уравнение $a_k x_{k-1} + b_k x_k + c_k x_{k+1} = f_k$, получаем
	\begin{align*}
	a_k (P_{k-1} x_k + Q_{k-1}) + b_k x_k + c_k x_{k+1} = f_{k}\\
	(a_k P_{k-1} + b_k) x_k = - c_k x_{k+1} + f_{k} - a_k Q_{k-1}\\
	\end{align*}
	Записывая это уравнение в виде прогоночного соотношения
	\[
	x_{k} = \frac{-c_k}{a_k P_{k-1} + b_k} x_{k+1} + \frac{f_k - a_k
Q_{k-1}}{a_k P_{k-1} + b_k},
	\]
	получаем рекуррентное соотношение для $P_k, Q_k$
	\[
	P_{k} = \frac{-c_k}{a_k P_{k-1} + b_k}, \qquad Q_{k} = \frac{f_k - a_k
Q_{k-1}}{a_k P_{k-1} + b_k}
	\]
\end{frame}

\begin{frame}\frametitle{Вычисление прогоночных коэффициентов}
	\[
	P_{k} = \frac{-c_k}{a_k P_{k-1} + b_k}, \qquad Q_{k} = \frac{f_k - a_k
Q_{k-1}}{a_k P_{k-1} + b_k}
	\]
	$P_1$ и $Q_1$ можно найти из первого уравнения $b_1 x_1 + c_1 x_2 = f_1$
	\[
	P_1 = \frac{-c_1}{b_1}, \quad Q_1 = \frac{f_1}{b_1}
	\]
	Заметим, что $P_{n} = 0$ и $x_n = Q_{n}$.
	Остальные неизвестные находятся с помощью прогоночного соотношения
	\[
	x_{k} = P_k x_{k+1} + Q_k, \quad k = n-1, n-2, \dots, 1
	\]
\end{frame}

\begin{frame}\frametitle{Свойства метода прогонки}
	Как и метод Гаусса, прогонка устойчива (в процессе прогонки не будут встречаться деления на числа близкие к 0) при наличии
	у системы диагонального преобладания, то есть
	$$|b_k| > |a_k| + |c_k|, k=\overline{1,n}.$$

	Арифметическая сложность прогонки составляет $O(n)$ действий, в то время как у метода Гаусса --- $O(n^3)$. Это одна из причин
	выделения прогонки как отдельного метода.
\end{frame}

\subsection{LU разложение}
\begin{frame}\frametitle{LU разложение}
	В результате применения метода Гаусса для матрица $\A$ вычисляется ее $\L\U$ разложение, то есть представление в виде произведения
	нижнетреугольной матрицы $\L$ на верхнетреугольную матрицу $\U$. При этом система
	\[
	\A\x = \b
	\]
	решается в два этапа
	\[
	\L\U \x = \b \Rightarrow \left\{
	\begin{array}{lcl}
	\L \y &=& \b\\
	\U \x &=& \y\\
	\end{array}
	\right.
	\]
	На каждом этапе прямой или обратной подстановкой решается система с треугольной матрицей.
\end{frame}

\begin{frame}\frametitle{Построение LU разложения}
	Рассмотрим, как строится $\L\U$ разложение заданной матрицы.

	Пусть матрица $\A$ имеет вид
	\[
	\A = \begin{pmatrix}
		a_{11}& \w^\tr\\
		\v & \A'\\
	\end{pmatrix}
	\]
	Тогда для матрицы $\A$ справедливо представление
	\[
	\A =
	\begin{pmatrix}
		1&0\\
		\v/a_{11}&\E
	\end{pmatrix}
	\begin{pmatrix}
		a_{11}&\w^\tr\\
		0&\A' - \v\w^\tr/a_{11}
	\end{pmatrix} =
	\begin{pmatrix}
		1&0\\
		\v/a_{11}&\L'
	\end{pmatrix}
	\begin{pmatrix}
		a_{11}&\w^\tr\\
		0&\U'
	\end{pmatrix}
	\]
	Здесь $\L'\U'$ --- $LU$-разложение матрицы $\A' -\v\w^\tr/a_{11}$
\end{frame}

\begin{frame}\frametitle{Пример}
	Рассмотрим построение $\L\U$ разложения на примере
	\[
	\A =
	\begin{pmatrix}
		1&1&1\\
		2&4&4\\
		3&7&10
	\end{pmatrix} =
	\begin{pmatrix}
		1&0&0\\
		2&1&0\\
		3&0&1
	\end{pmatrix}
	\begin{pmatrix}
		1&1&1\\
		0&4-2&4-2\\
		0&7-3&10-3
	\end{pmatrix}
	\fbox{=}
	\]
	Построим разложение для подматрицы $2\times2$
	\[
	\begin{pmatrix}
		4-2&4-2\\
		7-3&10-3
	\end{pmatrix} =
	\begin{pmatrix}
		2&2\\
		4&7
	\end{pmatrix} =
	\begin{pmatrix}
		1&0\\
		4/2&1
	\end{pmatrix}
	\begin{pmatrix}
		2&2\\
		0&7-4
	\end{pmatrix}
	\]
	Теперь можно записать разложение для $\A$
	\[
	\fbox{=}
	\begin{pmatrix}
		1&0&0\\
		2&1&0\\
		3&2&1
	\end{pmatrix}
	\begin{pmatrix}
		1&1&1\\
		0&2&2\\
		0&0&3
	\end{pmatrix}
	\]
\end{frame}

\section{Итерационные методы}
\subsection{Абстрактный метод простой итерации}
\begin{frame}\frametitle{Абстрактный процесс простой итерации}
	Рассмотрим абстрактный итерационный процесс
	\[
	\x_{k+1} = \B \x_k + \f,
	\]
	где $\B$ --- некоторая матрица, $\f$ --- некоторый вектор.

	Пусть данный процесс сходится к $\x^*$. Тогда
	\[
	\x^* = \B \x^* + \f
	\]

	Рассмотрим при каких ограничениях этот процесс сходится к $\x^*$
\end{frame}

\begin{frame}\frametitle{Достаточное условие сходимости}
	\centerline{$\x_{k+1} = \B \x_k + \f,$}
	Рассмотрим вектор ошибки $\r_k \equiv \x_k - \x^*$.
	\[
	\r_{k+1} = \x_{k+1}-\x^* = \B \x_k + \f-\B \x^* - \f = \B \r_k,
	\]
	\[
		\fbox{$\r_{k+1} = \B \r_k$}
	\]
	Таким образом, вектор ошибки за одну итерацию умножается на $\B$. Если \underline{какая-то} норма
	$\|\B\|_\bullet = q < 1$, то
	\[
	\|\r_{k+1}\|_\bullet = \|\B\r_k\|_\bullet \leq \|\B\|_\bullet \|\r_k\|_\bullet = q \| \r_k \|_\bullet
	\]
	Таким образом, норма невязки будет стремиться к нулю со скоростью геометрической прогрессии
	\[
	\|\r_k\|_\bullet \leq C q^k
	\]
	Это --- \emph{достаточное} условие сходимости процесса.

	Из-за эквивалентности норм в $\mathbb{R}^n$, для других норм будет справедливо то же самое, но с другой константой $C$
\end{frame}

\begin{frame}\frametitle{Критерий сходимости}
	\[
	\x_{k+1} = \B \x_k + \f,
	\]
	Для того чтобы процесс сходился при любом начальном приближении $\x_0$
	\emph{необходимо и достаточно},
	чтобы все собственные значения $\B$ (возможно комплексные) лежали внутри единичного круга
	\[
	|\lambda_i(\B)| < 1
	\]
	Если это условие не выполнено, то итерационный процесс может не сойтись при
некотором приближении. На практике это будет означать, что почти при всех
начальных приближениях он будет расходиться.
\end{frame}

\subsection{Метод простой итерации с параметром}
\begin{frame}\frametitle{Метод простой итерации с параметром $\tau$}
	Рассмотрим систему $\A\x = \b$. Построим такой итерационный процесс $\x_{k+1} = \B \x_k + \f$, чтобы
	его предел совпадал с решением системы.

	\pause
	Рассмотрим процесс с некоторым параметром $\tau \neq 0$
	\[
	\x_{k+1} = \x_k - \tau(\A\x_k - \b) = (\E - \tau \A) \x_k + \tau \b
	\]
	Ели данный процесс сходится, то он сходится к решению системы
	\[
	\x^* = \x^* - \tau(\A\x^* - \b) \Leftrightarrow \A \x^* = \b
	\]
\end{frame}

\begin{frame}\frametitle{Сходимость метода простой итерации с параметром $\tau$}
	Метод простой итерации соответствует процессу с $$\B = \E - \tau \A, \f = \tau \b$$
	\pause
	Собственные значения матрицы $\B$ связаны с собственными значениями матрицы $\A$ соотношением
	$$
	\lambda(\B) = 1 - \tau \lambda(\A)
	$$
	Необходимое и достаточное условие сходимости для любого приближения даются условием $|\lambda(\B)| < 1$
	$$
	|1 - \tau \lambda(\A)| < 1 \Leftrightarrow \left|\lambda(\A) -
\frac{1}{\tau}\right| < \frac{1}{|\tau|}
	$$
	Последнее условие означает, что все собственные числа матрицы $\A$ должны
лежать в круге с центром в точке $\tau^{-1}$ и радиуса $|\tau|^{-1}$
\end{frame}

\begin{frame}\frametitle{Оптимальный параметр для метода простой итерации}
	Допустим, что $\A = \A^\tr > 0$. Пусть собственные числа $\A$ лежат на
отрезке $[\lambda_{\min}, \lambda_{\max}]$
	\pause

	Итерационный процесс сходится со скоростью геометрической прогрессии
	\[
	\|\r_k\|_E < Cq^k,\quad q = \max |\lambda(\B)| = \|\B\|_E
	\]
	\pause
	Чтобы метод сходился необходимо, чтобы $0 < \tau < \frac{2}{\lambda_{\max}}$. Найдем скорость сходимости при фиксированном $\tau$.

	\pause
	Если собственные числа матрицы $\A$ лежат на отрезке $[\lambda_{\min},
\lambda_{\max}]$,
то собственные числа $\B$ --- на отрезке $[1-\tau \lambda_{\min}, 1-\tau
\lambda_{\max}]$.
	\[
	q = \max(|1-\tau \lambda_{\max}|, |1-\tau \lambda_{\min}|)
	\]
	Минимально возможное значение $q$ будет в случае
	\[|1-\tau \lambda_{\min}| = |1-\tau \lambda_{\max}|\]
	\[
	1 - \tau \lambda_{\min} = \tau \lambda_{\max} - 1, \quad \tau_\text{опт} =
\frac{2}{\lambda_{\min}+\lambda_{\max}}, \quad q_\text{опт} =
\frac{\lambda_{\max}-\lambda_{\min}}{\lambda_{\max}+\lambda_{\min}}
	\]
\end{frame}

\subsection{Методы Якоби и Зейделя}
\begin{frame}\frametitle{Метод Якоби}
	Пусть $\D = \mathop{diag}(a_{11}, a_{22}, \dots, a_{nn})$, то есть диагональ матрицы $\A$. 
	Метод Якоби можно трактовать как метод простой итерации для системы
	$$
	\D \x_{k+1} + (\A-\D) \x_k = \b
	$$

	В данном случае система легко решается относительно диагональных
неизвестных.
	\begin{equation*}
	\begin{array}{ccccccccc}
	a_{11} \textcolor{red}{x^{(k+1)}_{1}} &+& a_{12} x^{(k)}_{2}  &+& \dots &+& a_{1n} x^{(k)}_{n} &=& b_1\\
	a_{21} x^{(k)}_{1} &+& a_{22} \textcolor{red}{x^{(k+1)}_{2}} &+& \dots &+& a_{2n} x^{(k)}_{n} &=& b_2\\
	a_{31} x^{(k)}_{1} &+& a_{32} x^{(k)}_{2} &+& \dots &+& a_{1n} x^{(k)}_{n} &=& b_3\\
	\vdots\\
	a_{n1} x^{(k)}_{1} &+& a_{n2} x^{(k)}_{2} &+& \dots &+& a_{nn}
\textcolor{red}{x^{(k+1)}_{n}} &=& b_n
	\end{array}
	\end{equation*}
\end{frame}

\begin{frame}\frametitle{Критерий сходимости метода Якоби}
	Стандартный критерий $|\lambda(\B)| < 1$ можно в случае метода Якоби
записать в следующем виде: корни уравнения
	\[
		\operatorname{det} (\lambda \D + \A - \D) =
		\begin{vmatrix}
			\lambda a_{11} & a_{12} & \cdots & a_{1n}\\
			a_{11} & \lambda a_{22} & \cdots & a_{2n}\\
			&&\vdots\\
			a_{n1} & a_{n2} & \cdots & \lambda a_{nn}\\
		\end{vmatrix} = 0
	\]
	должны попасть в единичный круг. Фактически, это уравнение эквивалентно
характеристическому уравнению для матрицы $\B$
	\[
		\operatorname{det}(\B - \lambda \E) = 0.
	\]
\end{frame}

\begin{frame}\frametitle{Сходимость метода Якоби}
	Пусть матрица $\A$ имеет диагональное преобладание
	$|a_{ii}| \geq \sum_{j \neq i} |a_{ij}| + \delta |a_{ii}|, \quad i=\overline{1,n}$

	Матрица итерационного процесса $\B = \E - \D^{-1}\A$. Найдем ее $\|\cdot\|_\infty$ норму
	\[
	\|\B\|_\infty = \max_i\sum_j|b_{ij}| = \max_i \sum_{j\neq i} \left|-\frac{a_{ij}}{a_{ii}}\right| =
	\max_i \frac{\sum_{j\neq i} |a_{ij}|}{a_{ii}} \leq 1-\delta < 1
	\]

	Достаточное условие сходимости процесса выполнено
\end{frame}

\begin{frame}\frametitle{Метод Зейделя}
	Разобьем матрицу $\A$ на сумму двух матриц $\A = \L + \U$. (Это не матрицы
из LU разложения, а просто части матрицы $\A$).
	Матрица $\L$ будет содержать все элементы на главной диагонали и под ней, а $\U$ --- все элементы
	выше главной диагонали
	\[
	\L = \begin{pmatrix}
		a_{11}&0&\dots&0\\
		a_{21}&a_{22}&\dots&0\\
		\vdots&&\ddots&0\\
		a_{n1}&a_{n2}&\dots&a_{nn}
	\end{pmatrix}
	\quad
	\U = \begin{pmatrix}
		0&a_{12}&\dots&a_{1n}\\
		0&0&\dots&a_{2n}\\
		0&0&\ddots&\vdots\\
		0&0&0&0\\
	\end{pmatrix}
	\]

	Итерационный процесс записывается в виде
	\[
	\L \x_{k+1} + \U \x_k = \b
	\]
\end{frame}

\begin{frame}\frametitle{Критерий сходимости метода Зейделя}
	Стандартный критерий $|\lambda(\B)| < 1$ можно в случае метода Зейделя
записать в следующем виде: корни уравнения
	\[
		\operatorname{det} (\lambda \L + \U) =
		\begin{vmatrix}
			\lambda a_{11} & a_{12} & \cdots & a_{1n}\\
			\lambda a_{11} & \lambda a_{22} & \cdots & a_{2n}\\
			&&\vdots\\
			\lambda a_{n1} & \lambda a_{n2} & \cdots & \lambda a_{nn}\\
		\end{vmatrix} = 0
	\]
	должны попасть в единичный круг. Фактически, это уравнение эквивалентно
характеристическому уравнению для матрицы $\B$
	\[
		\operatorname{det}(\B - \lambda \E) = 0.
	\]
\end{frame}

\begin{frame}\frametitle{Метод Зейделя}
	$$
	\L \x_{k+1} + \U \x_k = \b
	$$
	Для вычисления следующего приближения $\x_{k+1}$ необходимо на каждом шаге решать систему с треугольной матрицей $\L$. 

	В данном случае треугольная система решается очень просто
	\begin{equation*}
	\begin{array}{ccccccccc}
	a_{11} \textcolor{red}{x^{(k+1)}_{1}} &+& a_{12} x^{(k)}_{2}  &+& \dots &+& a_{1n} x^{(k)}_{n} &=& b_1\\
	a_{21} \textcolor{red}{x^{(k+1)}_{1}} &+& a_{22} \textcolor{red}{x^{(k+1)}_{2}} &+& \dots &+& a_{2n} x^{(k)}_{n} &=& b_2\\
	a_{31} \textcolor{red}{x^{(k+1)}_{1}} &+& a_{32}
\textcolor{red}{x^{(k+1)}_{2}} &+& \dots &+& a_{1n} x^{(k)}_{n} &=& b_3\\
	\vdots\\
	a_{n1} \textcolor{red}{x^{(k+1)}_{1}} &+& a_{n2}
\textcolor{red}{x^{(k+1)}_{2}} &+& \dots &+& a_{nn}
\textcolor{red}{x^{(k+1)}_{n}} &=& b_n
	\end{array}
	\end{equation*}
	Если решать уравнения сверху вниз, то в каждом уравнении неизвестно ровно
одно значение $x^{(k+1)}_{j}$.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%                            %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain]
  \begin{center}
  {\Huge Спасибо за внимание!}
  \vspace{8ex}

  Цыбулин Иван

  e-mail: \colorhref{mailto:tsybulin@crec.mipt.ru}{tsybulin@crec.mipt.ru}
  \end{center}
\end{frame}

\end{document}
