\documentclass[professionalfonts,compress,unicode]{beamer}

\usepackage{amsmath,amssymb}
\usepackage[utf8]{inputenc}

\usepackage[russian]{babel}

\usepackage{multirow}
\usepackage{colortbl}

\usetheme{Warsaw}
\usecolortheme{uranix}

\setbeamertemplate{headline}
{%
  \begin{beamercolorbox}[sep=0.3cm,wd=\paperwidth]{section in head/foot}%
    \usebeamerfont{frametitle}%
    \vbox{}\vskip-1ex%
    \strut\insertsectionhead\strut\par%
    \vskip-1ex%
  \end{beamercolorbox}%
}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\graphicspath{{images//}}

\title[СЛАУ]{Системы линейных алгебраических уравнений\\Часть 2. Прямые и итерационные методы решения}
\author[Цыбулин И.В.]{Скалько Юрий Иванович\\
\textbf{Цыбулин Иван}
}
\date{}
%\vspace{0.3cm}

\begin{document}

{
\setbeamertemplate{headline}[default]
\frame{
\titlepage
}

%\frame{
%\frametitle{Содержание}
%\small
%%\tiny
%\tableofcontents
%}
}

\newcommand\myframe[2]{\subsection{#1}\frame{\frametitle{#1}{#2}}}

\section{ }

%\myframe{Материалы по курсу вычислительной математики}
%{
%\begin{itemize}
	%\item 
		%Материалы курса (методички, лекции, учебники и др.) можно найти 
		%на сайте кафедры вычислительной математики
		%{\color{blue} http://crec.mipt.ru/study/materials/compmath/}
	%\item 
		%Любые вопросы по курсу (и не только) можно присылать на почтовый ящик
		%{\color{blue} tsybulinhome@gmail.com}
%\end{itemize}
%}

\def\A{{\bf A}}
\def\B{{\bf B}}
\def\C{{\bf C}}
\def\D{{\bf D}}
\def\E{{\bf E}}
\def\L{{\bf L}}
\def\U{{\bf U}}
\def\S{{\bf S}}
\def\J{{\bf J}}
\def\x{{\bf x}}
\def\y{{\bf y}}
\def\v{{\bf v}}
\def\w{{\bf w}}
\def\f{{\bf f}}
\def\b{{\bf b}}
\def\r{{\bf r}}
\def\p{{\bf p}}
\def\z{{\bf z}}


\section{Методы решения СЛАУ}
\myframe{Прямые и итерационные методы}
{
	Численные методы решения СЛАУ делятся на 2 типа:
	\begin{itemize}
	\item{Прямые} --- решение системы получается за конечное число действий
	\item{Итерационные} --- решение строится в виде последовательности приближений к решению.
	При достижении заданной точности итерации прекращают
	\end{itemize}
}

\section{Метод Гаусса}
\myframe{Метод Гаусса}
{
	В основе метода лежит последовательное исключение неизвестных. Метод Гаусса состоит из 
	\emph{прямого хода}, на котором матрица системы элементарными преобразованиями строк преобразуется
	к верхнетреугольному виду, и \emph{обратного хода}, когда полученная треугольная система решается 
	подстановкой.
	
	\pause
	На практике, иногда требуется переставлять строки или столбцы матрицы, чтобы избежать деления на ноль. 
	Также, переставляя стоки и столбцы можно улучшить устойчивость метода, то есть чувствительность к ошибкам округления
}

\myframe{Метод Гаусса}
{
	Рассмотрим систему
	$$
	\begin{pmatrix}
		10^{-3}&1\\
		1&-1
	\end{pmatrix}
	\begin{pmatrix}
		x_1\\x_2
	\end{pmatrix} = 
	\begin{pmatrix}
		2\\1
	\end{pmatrix}
	\qquad
	\begin{pmatrix}
		x_1\\x_2
	\end{pmatrix} \approx
	\begin{pmatrix}
		2.997003\\
		1.997003\\
	\end{pmatrix}	
	$$
	
	Будем решать ее методом Гаусса, используя в вычислениях только три значащие цифры
	$$
	\left(
	\begin{array}{@{}cc|c@{}}
		10^{-3}&1&2\\
		1&-1&1
	\end{array}
	\right)
	\sim
	\left(
	\begin{array}{@{}cc|c@{}}
		1&10^3&2\cdot10^3\\
		1&-1&1
	\end{array}
	\right)
	\stackrel{!}{\sim}
	\left(
	\begin{array}{@{}cc|c@{}}
		1&10^3&2\cdot10^3\\
		0&-10^3&-2\cdot10^3\\
	\end{array}
	\right)
	\sim	
	$$
	$$
	\left(
	\begin{array}{@{}cc|c@{}}
		1&10^3&2\cdot10^3\\
		0&-10^3&-2\cdot10^3\\
	\end{array}
	\right)
	\sim
	\left(
	\begin{array}{@{}cc|c@{}}
		1&10^3&2\cdot10^3\\
		0&1&2\\
	\end{array}
	\right)
	\sim
	\left(
	\begin{array}{@{}cc|c@{}}
		1&0&0\\
		0&1&2\\
	\end{array}
	\right)		
	$$
	Полученное решение значительно отличается от точного.
}

\myframe{Метод Гаусса с выделением главного элемента}
{
	Данную проблему можно устранить, если в качестве ведущего элемента (того на который делится очередная строка)
	выбирать наибольший по модулю элемент в оставшейся подматрице. Такой метод называется методом Гаусса с выделением главного элемента.
	
	Применим его к той же системе
	$$
	\left(
	\begin{array}{@{}cc|c@{}}
		10^{-3}&1&2\\
		\fbox{1}&-1&1
	\end{array}
	\right)
	\stackrel{!}{\sim}
	\left(
	\begin{array}{@{}cc|c@{}}
		0&\fbox{1}&2\\
		1&-1&1\\
	\end{array}
	\right)
	\sim
	\left(
	\begin{array}{@{}cc|c@{}}
		0&1&2\\
		1&0&3\\	
	\end{array}
	\right)
	\sim
	\left(
	\begin{array}{@{}cc|c@{}}
		1&0&3\\	
		0&1&2\\
	\end{array}
	\right)
$$

	Теперь отличие от точного решения не превосходит $3 \cdot 10^{-3}$, что даже меньше погрешности вычислений.
}

\myframe{Обусловенность}
{
	Ислледуем, насколько плохо обусловлена матрица этой системы
	$$
	\A=\begin{pmatrix}
		10^{-3}&1\\
		1&-1
	\end{pmatrix}
	\qquad
	\A^{-1}=\frac{1}{1001}\begin{pmatrix}
		1000&1000\\
		1000&-1
	\end{pmatrix}	
	$$
	\pause
	$$
	\mu_\infty(\A) = \|\A\|_\infty\|\A^{-1}\|_\infty = 2 \cdot \frac{2000}{1001} \approx 4
	$$
	
	Число обусловленности матрицы невелико, и хорошим численным методом можно решить данную систему с относительной ошибкой,
	не превосходящей $4 \frac{\delta \b}{\b}$. 
	
	Следовательно, потеря точности при решении методом Гаусса без выбора главного элемента связана 
	с самим методом, а не с плохой постановкой задачи. К тому же, метод Гаусса с выбором главного
	элемента нашел решение, погрешность которого вполне соответствует неустранимой ошибке
}

\myframe{Устойчивость}
{
	Проблема метода Гаусса заключается в накоплении ошибок округления. Эта проблема возникает всегда при 
	операциях с числами разных знаков или разных порядков. Метод с выбором главного элемента позволяет уменьшить ошибку,
	но не всегда этого оказывается достаточно.
	
	\begin{block}{Теорема об устойчивости метода Гаусса}
		Если матрица $\A$ имеет диагональное преобладание
		$$
			|a_{ii}| \geq \sum_{j \neq i} |a_{ij}|,\quad i = \overline{1,n},
		$$
		то при решении методом Гаусса погрешность не превосходит неустранимой погрешности, то есть
		метод Гаусса \emph{устойчив}
	\end{block}
}

\myframe{Прогонка}
{
	Существует вариант метода Гаусса для важных в приложениях \emph{трехдиагональных} систем линейных уравнений
	$$
	\left\{
	\begin{array}{@{}c@{}c@{}c@{}c@{}c@{}c@{}ccc}
	b_1 x_1 &+& c_1 x_2 &&   && &=& f_1\\
	a_2 x_1 &+& b_2 x_2 &+& c_2 x_3 && &=& f_2\\
	&&&\ddots\\
	&& a_{n-1} x_{n-2} &+& b_{n-1} x_{n-1} &+& c_{n-1} x_n &=& f_{n-1}\\
	&&&& a_{n} x_{n-1} &+& b_{n} x_{n} &=& f_{n}\\
	\end{array}
	\right.
	$$
}

\myframe{Вычисление прогоночных коэффициентов}
{
	Решение ищется в виде прогоночного соотношения
	$$
	x_{i-1} = P_{i} x_{i} + Q_{i}
	$$
	При подстановке его в уравнение $a_k x_{k-1} + b_k x_k + c_k x_{k+1} = f_k$, получаем
	\begin{align*}
	a_k (P_k x_k + Q_k) + b_k x_k + c_k x_{k+1} = f_{k}\\
	(a_k P_k + b_k) x_k = - c_k x_{k+1} + f_{k} - a_k Q_k\\
	\end{align*}
	Записывая это уравнение в виде прогоночного соотношения
	$$
	x_{k} = \frac{-c_k}{a_k P_k + b_k} x_{k+1} + \frac{f_k - a_k Q_k}{a_k P_k + b_k},
	$$
	получаем рекуррентное соотношение для $P_k, Q_k$
	$$
	P_{k+1} = \frac{-c_k}{a_k P_k + b_k}, \qquad Q_{k+1} = \frac{f_k - a_k Q_k}{a_k P_k + b_k}
	$$
}

\myframe{Вычисление прогоночных коэффициентов}
{
	$$
	P_{k+1} = \frac{-c_k}{a_k P_k + b_k}, \qquad Q_{k+1} = \frac{f_k - a_k Q_k}{a_k P_k + b_k}
	$$
	$P_2$ и $Q_2$ можно найти из первого уравнения $b_1 x_1 + c_1 x_2 = f_1$
	$$
	P_2 = \frac{-c_1}{b_1}, \quad Q_2 = \frac{f_1}{b_1}
	$$
	%(формально, можно просто положить $P_1 = Q_1 = 0$)
	%
	Заметим, что $P_{n+1} = 0$ и $x_n = Q_{n+1}$.
	Остальные неизвестные находятся с помощью прогоночного соотношения
	$$
	x_{k-1} = P_k x_k + Q_k, \quad k = n, n-1, n-2, \dots, 2
	$$
}

\myframe{Прогонка}
{
	Как и метод Гаусса, прогонка устойчива (в процессе прогонки не будут встречаться деления на числа близкие к 0) при наличии 
	у системы диагонального преобладания, то есть 
	$$|b_k| > |a_k| + |c_k|, k=\overline{1,n}.$$
	
	Арифметическая сложность прогонки составляет $O(n)$ действий, в то время как у метода Гаусса --- $O(n^3)$. Это одна из причин 
	выделения прогонки как отдельного метода.
}

\myframe{LU факторизация}
{
	В результате применения метода Гаусса для матрица $\A$ вычисляется ее $\L\U$ разложение, то есть представление в виде произведения
	нижнетреугольной матрицы $\L$ на верхнетреугольную матрицу $\U$. При этом система
	$$
	\A\x = \b
	$$
	решается в два этапа
	$$
	\L\U \x = \b \Rightarrow \left\{ 
	\begin{array}{lcl}
	\L \y &=& \b\\
	\U \x &=& \y\\
	\end{array}
	\right.
	$$
	На каждом этапе решается система с треугольной матрицей.
}

\myframe{LU факторизация}
{
	Рассмотрим, как строится $\L\U$ разложение заданной матрицы.
	
	Пусть матрица $\A$ имеет вид
	$$
	\A = \begin{pmatrix}
		a_{11}& \w^T\\
		\v & \A'\\
	\end{pmatrix}
	$$
	Тогда для матрицы $\A$ справедливо представление
	$$
	\A = 
	\begin{pmatrix}
		1&0\\
		\v/a_{11}&\E
	\end{pmatrix}
	\begin{pmatrix}
		a_{11}&\w^T\\
		0&\A' - \v\w^T/a_{11}
	\end{pmatrix} = 
	\begin{pmatrix}
		1&0\\
		\v/a_{11}&\L'
	\end{pmatrix}
	\begin{pmatrix}
		a_{11}&\w^T\\
		0&\U'
	\end{pmatrix}
	$$
	Здесь $\L'\U'$ - $LU$-разложение матрицы $\A' -\v\w^T/a_{11}$
}

\myframe{LU-факторизация}
{
	Рассмотрим как стоится $\L\U$ факторизация на примере
	$$
	\A = 
	\begin{pmatrix}
		1&1&1\\
		2&4&4\\
		3&7&10
	\end{pmatrix} =
	\begin{pmatrix}
		1&0&0\\
		2&1&0\\
		3&0&1
	\end{pmatrix}
	\begin{pmatrix}
		1&1&1\\
		0&4-2&4-2\\
		0&7-3&10-3
	\end{pmatrix}
	\fbox{=}
	$$
	Построим разложение для подматрицы $2\times2$
	$$
	\begin{pmatrix}
		4-2&4-2\\
		7-3&10-3
	\end{pmatrix} = 
	\begin{pmatrix}
		2&2\\
		4&7
	\end{pmatrix} = 
	\begin{pmatrix}
		1&0\\
		4/2&1
	\end{pmatrix}
	\begin{pmatrix}
		2&2\\
		0&7-4
	\end{pmatrix}
	$$
	Теперь можно записать разложение для $\A$
	$$
	\fbox{=}
	\begin{pmatrix}
		1&0&0\\
		2&1&0\\
		3&2&1
	\end{pmatrix}
	\begin{pmatrix}
		1&1&1\\
		0&2&2\\
		0&0&3
	\end{pmatrix}
	$$
}

%\section{Метод сопряженных градиентов}
%\myframe{Метод сопряженных градиентов}
%{
%	Метод сопряженных градиентов относят как к прямым так и к итерационным методам. 
%	В основе метода лежит поиск минимума функции
%	$$
%	f(\x) = \frac{1}{2}\x^T \A \x - \b^T \x, \qquad A = A^T > 0
%	$$
%	В точке $\x^*$ ($\A\x^* = \b$)
%	$$
%	\nabla f(\x^*) = \A\x^* - \b = 0
%	$$
%	выполнено необходимое и достаточное условие минимума квадратичной функции $f(\x)$.
%}
%
%\myframe{Алгоритм метода}
%{
%	Метод стартует с некоторого произвольного приближения $\x_0$. 
%	На каждом шаге метода вычисляется невязка уравнения
%	$$
%	\r_k = \b - \A \x_k,
%	$$
%	направление
%	$$
%	\p_{k+1} = \r_k - \sum_{i=1}^k \frac{\p_i^T \A \r_k}{\p_i^T \A \p_i} \p_i
%	$$
%	и параметр
%	$$
%	\alpha_{k+1} = \frac{\p_{k+1}^T \r_k}{\p_{k+1}^T \A \p_{k+1}}.
%	$$
%	
%	Следующее приближение $\x_{k+1}$ находится как
%	$$
%	\x_{k+1} = \x_{k} + \alpha_{k+1}\p_{k+1}
%	$$
%}
%
%\myframe{Метод сопряженных градиентов}
%{
%	Метод сопряженных градиентов можно рассматривать как метод построения 
%	приближения $\x_{k+1}$ по имеющимся уже $\x_0, \x_1, \x_2, \dots \x_k$.
%	Можно доказать, что для \emph{точных} вычислений начиная с $k = n$ приближения
%	метода сопряженных градиентов будут совпадать с точным решением системы $\A\x = \b$.
%	По этой причине метод можно относить к прямым.
%	
%	Но на практике, из-за ошибок округления, даже через $n$ итераций значения $\x_k$
%	продолжают изменяться. Из-за этого метод сопряженных градиентов используют как итерационный,
%	причем количество необходимых итераций для достижения приемлимой точности обычно намного меньше $n$.
%}
%
%\myframe{Свойства метода сопряженных градиентов}
%{
%	Метод сопряженных градиентов имеет ряд достоинств:
%	\begin{itemize}
%	\item может использоваться как прямой и как итерационный
%	\item не требует явного задания матрицы $\A$, необходимо лишь уметь вычислять $\A\x$ для любого $\x$
%	\end{itemize}
%	\pause
%	и недостатков:
%	\begin{itemize}
%	\item метод чувствителен к ошибкам округления и плохо обусловленным системам. 
%	Возможна расходимость метода для плохо обусловленных систем при недостаточной точности вычислений
%	\end{itemize}	
%}
%
\section{Вспомогательные методы}
\myframe{Симметризация}
{
%	Метод сопряженных градиентов решает системы только с симметричными положительно определенными матрицами. 
	Предположим, имеется эффективный метод решения системы с симметричной положительно определенной матрицей.
	Что делать если система $\A\x = \b$ не такая?
	\pause
	
	Умножим эту систему слева на $\A^T$
	$$
	\A^T \A \x = \A^T \b
	$$
	
	Полученная система имеет уже положительную матрицу $\A^T\A$. 
	
	\pause
	Но число обусловленности такой матрицы
	$$
	\mu(\A^T\A) \lesssim \mu(\A)^2
	$$
	
	То есть такая операция сильно ухудшает обусловленность матрицы.
}

\myframe{Предобуславливание}
{
	Пусть имеется система $\A\x = \b$ с плохо обусловленной матрицей $\A$. Плохо обусловленная матрица
	может стать препятствием при решении системы численным метдом. Поэтому имеет смысл попытаться снизить
	обусловленность системы. 
	\pause
	
	Предположим, что имеется матрица $\C \approx \A^{-1}$. Домножим систему на $\C$
	$$
	\C\A\x = \C\b
	$$
	
	Поскольку $\C\A \approx \E$, число обусловленности $\mu(\C\A) \approx \mu(\E) = 1$. 
	Полученную систему можно решать численным методом.
	Такая матрица $\C$ называется 
	предобуславливателем, а сама операция домножения на $\C$ --- предобуславливанием системы.
}

\section{Итерационные методы}
\myframe{Абстрактный процесс простой итерации}
{
	Рассмотрим абстрактный итерационный процесс
	$$
	\x_{k+1} = \B \x_k + \f,
	$$
	где $\B$ --- некоторая матрица, $\f$ --- некоторый вектор.
	
	Пусть данный процесс сходится к $\x^*$. Тогда 
	$$
	\x^* = \B \x^* + \f
	$$
	
	Рассмотрим при каких ограничениях этот процесс сходится к $\x^*$
}

\myframe{Абстрактный процесс простой итерации}
{
	$$
	\x_{k+1} = \B \x_k + \f,
	$$
	Рассмотрим невязку $\r_k = \x_k - \x^*$. 
	$$
	\r_{k+1} = \x_{k+1}-\x^* = \B \x_k + \f-\B \x^* - \f = \B \r_k,
	$$
	
	Таким образом, невязка за одну итерацию умножается на $\B$. Если \underline{какая-то} норма 
	$\|\B\|_\bullet = q < 1$, то 
	$$
	\|\r_{k+1}\|_\bullet = \|\B\r_k\|_\bullet \leq \|\B\|_\bullet \|\r_k\|_\bullet = q \| \r_k \|_\bullet
	$$
	Таким образом, норма невязки будет стремиться к нулю со скоростью геометрической прогрессии
	$$
	\|\r_k\|_\bullet \leq C q^k
	$$
	Это --- \emph{достаточное} условие сходимости процесса.

	Из-за эквивалентности норм в $\mathbb{R}^n$, для других норм будет справедливо то же самое, но с другой константой $C$
}

\myframe{Абстрактный процесс простой итерации}
{
	$$
	\x_{k+1} = \B \x_k + \f,
	$$
	Для того чтобы процесс сходился при любом начальном приближении $\x_0$ 
	\emph{необходимо и достаточно},
	чтобы все собственные значения $\B$ (возможно комплексные) лежали внутри единичного круга
	$$
	|\lambda_i(\B)| \leq q < 1
	$$
}

%\myframe{Доказательство достаточного условия}
%{
%	Рассмотрим представление матрицы $\B$ в виде
%	$$
%	\B = \S \J \S^{-1},
%	$$
%	где $\J$ --- Жорданова форма матрицы $\B$. $\J = \mathop{diag}(\J_1,\J_2,\dots,\J_t)$
%	$$
%	\B^k = \S \J \S^{-1} \S \J \S^{-1} \cdots \S \J \S^{-1} = \S \J^k \S^{-1}
%	$$
%	При возведении Жордановой клетки размера $r$ в степень $k$ получается матрица
%	$$
%	\J_i = \begin{pmatrix}
%		\lambda_i&1\\
%		&\lambda_i&1\\
%		&&\ddots&1\\
%		&&&\lambda
%	\end{pmatrix}
%	\quad
%	\J_i^k = \begin{pmatrix}
%		\lambda_i^k&C^1_k \lambda_i^{k-1}&\dots&C^{r-1}_k \lambda^{k-r+1}\\
%		&\lambda_i^k&C^1_k \lambda_i^{k-1}\\
%		&&\ddots&C^1_k \lambda_i^{k-1}\\
%		&&&\lambda^k
%	\end{pmatrix}
%	$$	
%}
%\myframe{Доказательство достаточного условия}
%{
%	Очень грубо можно оценить 
%	$$\|\J^k\|_\infty = \max_i{\|\J_i^k\|} \leq r k^{r-1} |\lambda|^{k-r+1},$$
%	где $\lambda, r$ относятся к Жордановой клетке с максимальным по модулю $\lambda_i$.
%	
%	Теперь можно оценить норму невязки
%	$$
%	\|\r_k\|_\infty \leq \|\S\|_\infty \|\S^{-1}\|_\infty \|\J^k\|_\infty \|\r_0\|_\infty \leq C k^{r-1} |\lambda|^k
%	$$
%	
%	Если $|\lambda| < 1$, то норма невязки стремится к нулю и процесс сходится.
%	
%}
%\myframe{Доказательство необходимого условия}
%{
%	Предположим, что у $\B$ есть собственное число $\lambda: |\lambda| \geq 1$. 
%	Пусть $\z$ --- соответствующий (комплкснозначный) собственный вектор
%	
%	Допустим, что итерационные процессы с $\r_0 = \mathop{Re}\z$ и
%	$\r_0 = \mathop{Im}\z$ сходятся
%	$$
%	\B^k \mathop{Re} \z \rightarrow 0
%	$$
%	$$
%	\B^k \mathop{Im} \z \rightarrow 0
%	$$
%	Тогда в норме $\|\z\|_H = \sqrt{(\bar{\z},\z)}$ будет выполнено
%	$$
%	\B^k (\mathop{Re} \z + i \mathop{Im} \z) \rightarrow 0
%	$$
%	Но
%	$$
%	\|\B^k (\mathop{Re} \z + i \mathop{Im} \z)\|_H = \|\B^k \z\|_H = |\lambda|^k \|\z\|_H \not \rightarrow 0
%	$$
%	Противоречие, оба процесса сходиться не могут
%}

\myframe{Метод простой итерации с параметром $\tau$}
{
	Рассмотрим систему $\A\x = \b$. Построим такой итерационный процесс $\x_{k+1} = \B \x_k + \f$, чтобы 
	его предел совпадал с решением системы.
	
	\pause
	Рассмотрим процесс с некоторым параметром $\tau$
	$$
	x_{k+1} = x_k - \tau(\A\x_k - \b) = (\E - \tau \A) \x_k + \tau \b
	$$
	Ели данный процесс сходится, то он сходится к решению системы
}

\myframe{Сходимость метода простой итерации с параметром $\tau$}
{
	Метод простой итерации соответствует процессу с $$\B = \E - \tau \A, \f = \tau \b$$
	\pause
	Собственные значения матрицы $\B$ связаны с собственными значениями матрицы $\A$ соотношением
	$$
	\lambda(\B) = 1 - \tau \lambda(\A)
	$$
	Необходимое и достаточное условие сходимости для любого приближения даются условием $|\lambda(\B)| < 1$
	$$
	|1 - \tau \lambda(\A)| < 1 \Leftrightarrow \left|\lambda(\A) - \frac{1}{\tau}\right| < \frac{1}{\tau}
	$$
	Последнее условие означает, что все собственные числа матрицы $\A$ должны лежать в круге с центром в точке $\tau^{-1}$ и радиуса $\tau^{-1}$
}

%\myframe{Оптимальный параметр для метода простой итерации}
%{
%	Допустим, что $\A = \A^T > 0$. Пусть собственные числа $\A$ лежат на отрезке $[\lambda, \Lambda]$
%	\pause
%	
%	Итерационный процесс сходится со скоростью геометрической прогрессии
%	$$
%	\|\r_k\| < Cq^k,\quad q = \max |\lambda(\B)|
%	$$
%	\pause
%	Чтобы метод сходился, необходимо, чтобы $0 < \tau < \frac{2}{\Lambda}$. Найдем скорость сходимости при фиксированном $\tau$.
%	
%	\pause
%	Если собственные числа матрицы $\A$ лежат на отрезке $[\lambda, \Lambda]$, то собственные числа $\B$ --- на отрезке $[1-\tau \Lambda, 1-\tau \lambda]$.
%	$$
%	q = \max(|1-\tau \Lambda|, |1-\tau \lambda|)
%	$$
%	Минимально возможное значение $q$ будет в случае 
%	$$|1-\tau \Lambda| = |1-\tau \lambda|$$
%	$$
%	1 - \tau \lambda = \tau \Lambda - 1, \quad \tau_\text{опт} = \frac{2}{\Lambda+\lambda}, \quad q_\text{опт} = \frac{\Lambda-\lambda}{\Lambda+\lambda}
%	$$
%}
%
\myframe{Метод Якоби}
{
	Пусть $\D = \mathop{diag}(a_{11}, a_{22}, \dots, a_{nn})$, то есть диагональ матрицы $\A$. 
	Метод Якоби можно трактовать как метод простой итерации для системы
	$$
	\D^{-1} \A \x = \D^{-1} \b
	$$
	
	Матрица $\D^{-1}$ выступает в роли предобуславливателя системы. 
	Запишем метод простой итерации c $\tau = 1$ для этой системы 
	$$
	\x_{k+1} = (\E - \D^{-1} \A) \x_k + \D^{-1} \b = \D^{-1} \left((\D - \A) \x_k + \b\right)
	$$
}

\myframe{Сходимость метода Якоби}
{
	Пусть матрица $\A$ имеет диагональное преобладание
	$|a_{ii}| \geq \sum_{j \neq i} |a_{ij}| + \delta |a_{ii}|, \quad i=\overline{1,n}$
	
	Матрица итерационного процесса $\B = \E - \D^{-1}\A$. Найдем ее $\|\cdot\|_\infty$ норму
	$$
	\|\B\|_\infty = \max_i\sum_j|b_{ij}| = \max_i \sum_{j\neq i} \left|-\frac{a_{ij}}{a_{ii}}\right| = 
	\max_i \frac{\sum_{j\neq i} |a_{ij}|}{a_{ii}} \leq 1-\delta < 1
	$$
	
	Достаточное условие сходимости процесса выполнено
}

\myframe{Метод Зейделя}
{
	Разобьем матрицу $\A$ на сумму двух матриц $\A = \L + \U$. 
	Матрица $\L$ будет содержать все элементы на главной диагонали и под ней, а $\U$ --- все элементы 
	выше главной диагонали
	$$
	\L = \begin{pmatrix}
		a_{11}&0&\dots&0\\
		a_{21}&a_{22}&\dots&0\\
		\vdots&&\ddots&0\\
		a_{n1}&a_{n2}&\dots&a_{nn}
	\end{pmatrix}
	\quad
	\U = \begin{pmatrix}
		0&a_{12}&\dots&a_{1n}\\
		0&0&\dots&a_{2n}\\
		0&0&\ddots&\vdots\\
		0&0&0&0\\
	\end{pmatrix}
	$$
	
	Итерационный процесс записывается в виде
	$$
	\L \x_{k+1} + \U \x_k = \b
	$$
}

\myframe{Метод Зейделя}
{
	$$
	\L \x_{k+1} + \U \x_k = \b
	$$
	Для вычисления следующего приближения $\x_{k+1}$ необходимо на каждом шаге решать систему с треугольной матрицей $\L$. 
	
	В данном случае треугольная система решается очень просто
	\begin{equation*}
	\begin{array}{ccccccccc}
	a_{11} \fbox{$x_{k+1,1}$} &+& a_{12} x_{k,2}  &+& \dots &+& a_{1n} x_{k,n} &=& b_1\\
	a_{21} x_{k+1,1} &+& a_{22} \fbox{$x_{k+1,2}$} &+& \dots &+& a_{2n} x_{k,n} &=& b_2\\
	a_{31} x_{k+1,1} &+& a_{32} x_{k+1,2} &+& \dots &+& a_{1n} x_{k,n} &=& b_3\\
	\vdots\\
	a_{n1} x_{k+1,1} &+& a_{n2} x_{k+1,2} &+& \dots &+& a_{nn} \fbox{$x_{k+1,n}$} &=& b_n
	\end{array}
	\end{equation*}
	Если решать уравнения сверху вниз, то в каждом уравнении неизвестно ровно одно значение $x_{k+1,j}$.
}

\myframe{Сходимость метода Зейделя}
{
	Запишем итерационный процесс метода Зейделя в форме $\x_{k+1} = \B \x_k + \f$
	$$
	\x_{k+1} = \L^{-1} (-\U \x_k + \b) = -\L^{-1} \U \x_k + \L^{-1} \b
	$$
	Пусть матрица $\A = \A^T > 0$. Тогда $\L = \U^T + \D$
	Пусть $\lambda, \x$ - собственное число и соответствующий вектор матрицы $\B = -\L^{-1} \U$
	$$
	\B \x = \lambda \x
	$$
	$$
	-\L^{-1} \U \x = \lambda \x, \quad -\U \x = \lambda \L \x
	$$
	Умножим это равенство слева на $\x^T$
	$$
	-\x^T \U \x = \lambda (\x^T \D \x + \x^T \U^T \x) = \lambda (\x^T \D \x + \x^T \U \x) \Rightarrow | \lambda | < 1
	$$
	Все собственные числа $\B$ лежат в единичном круге.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%                            %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{
\setbeamertemplate{headline}[default] 
\frame{
	\begin{center}
	{\Huge Спасибо за внимание!}
	\end{center}
	\bigskip
	\begin{center}
	{\color{blue}{tsybulin@crec.mipt.ru}}
	\end{center}
	}
}

\end{document}
